[project]
name = "vllm-distributed"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.13"
dependencies = [
    "nvidia-cuda-runtime-cu12>=12.8.90",
    "ray[default]>=2.53.0",
    "vllm==0.13.0",
]

[project.scripts]
head = "ray start --head --port 6379"
worker = "ray start --address 192.168.20.1:6379"
vllm = "vllm serve Qwen/Qwen3-0.6B --distributed-executor-backend ray --tensor-parallel-size 1"
